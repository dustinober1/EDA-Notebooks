{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Import required libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import StratifiedKFold, train_test_split\nfrom sklearn.metrics import roc_auc_score, classification_report, confusion_matrix\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Suppress TensorFlow warnings\nimport os\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n\n# Transformers and PyTorch\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.optim import AdamW\nfrom transformers import (\n    DistilBertTokenizer, \n    DistilBertForSequenceClassification,\n    get_linear_schedule_with_warmup\n)\nfrom tqdm.auto import tqdm\n\nprint('Libraries imported successfully!')\nprint(f'PyTorch version: {torch.__version__}')\nprint(f'CUDA available: {torch.cuda.is_available()}')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load competition data\ntrain_df = pd.read_csv('/kaggle/input/mercor-ai-detection/train.csv')\ntest_df = pd.read_csv('/kaggle/input/mercor-ai-detection/test.csv')\nsample_sub = pd.read_csv('/kaggle/input/mercor-ai-detection/sample_submission.csv')\n\n# Load large AI/Human dataset\ndataset2 = pd.read_csv(\"/kaggle/input/ai-vs-human-text/AI_Human.csv\")\n\nprint(f\"Competition train shape: {train_df.shape}\")\nprint(f\"Competition test shape: {test_df.shape}\")\nprint(f\"AI/Human dataset shape: {dataset2.shape}\")\n\nprint(\"\\nAI/Human dataset preview:\")\nprint(dataset2.head())\nprint(\"\\nAI/Human label distribution:\")\nprint(dataset2['generated'].value_counts())\nprint(dataset2['generated'].value_counts(normalize=True))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.utils import resample\n\n# Prepare large dataset\nX_large = dataset2['text'].values\ny_large = dataset2['generated'].values.astype(int)\n\nprint(\"=\"*60)\nprint(\"Original Dataset Distribution\")\nprint(\"=\"*60)\nprint(f\"Total samples: {len(X_large)}\")\nprint(f\"AI-generated: {sum(y_large)} ({sum(y_large)/len(y_large)*100:.2f}%)\")\nprint(f\"Human-written: {len(y_large)-sum(y_large)} ({(len(y_large)-sum(y_large))/len(y_large)*100:.2f}%)\")\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"Balancing Dataset - Undersampling\")\nprint(\"=\"*60)\n\n# Separate by class\nhuman_mask = y_large == 0\nai_mask = y_large == 1\n\nX_human = X_large[human_mask]\nX_ai = X_large[ai_mask]\n\nprint(f\"\\nBefore balancing:\")\nprint(f\"Human: {len(X_human)} samples\")\nprint(f\"AI: {len(X_ai)} samples\")\n\n# Undersample majority class (human) to match minority class (AI)\nX_human_downsampled = resample(\n    X_human,\n    replace=False,\n    n_samples=len(X_ai),\n    random_state=42\n)\ny_human_downsampled = np.zeros(len(X_ai), dtype=int)\ny_ai = np.ones(len(X_ai), dtype=int)\n\n# Combine balanced data\nX_large = np.concatenate([X_human_downsampled, X_ai])\ny_large = np.concatenate([y_human_downsampled, y_ai])\n\n# Shuffle\nshuffle_idx = np.random.RandomState(42).permutation(len(X_large))\nX_large = X_large[shuffle_idx]\ny_large = y_large[shuffle_idx]\n\nprint(f\"\\nAfter balancing:\")\nprint(f\"Total samples: {len(X_large)}\")\nprint(f\"Human: {sum(y_large == 0)} ({sum(y_large == 0)/len(y_large)*100:.2f}%)\")\nprint(f\"AI: {sum(y_large == 1)} ({sum(y_large == 1)/len(y_large)*100:.2f}%)\")\nprint(\"=\"*60)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load tokenizer\nmodel_path = r\"/kaggle/input/distillbert-base-uncased/transformers/default/1/distilbert-base-uncased\"\ntokenizer = DistilBertTokenizer.from_pretrained(model_path)\nprint(f\"Tokenizer loaded from {model_path}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class CheatingDetectionDataset(Dataset):\n    def __init__(self, texts, labels=None, tokenizer=None, max_length=512):\n        self.texts = texts\n        self.labels = labels\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n    \n    def __len__(self):\n        return len(self.texts)\n    \n    def __getitem__(self, idx):\n        text = str(self.texts[idx])\n        \n        encoding = self.tokenizer(\n            text,\n            add_special_tokens=True,\n            max_length=self.max_length,\n            padding='max_length',\n            truncation=True,\n            return_attention_mask=True,\n            return_tensors='pt'\n        )\n        \n        item = {\n            'input_ids': encoding['input_ids'].flatten(),\n            'attention_mask': encoding['attention_mask'].flatten()\n        }\n        \n        if self.labels is not None:\n            item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long)\n        \n        return item\n\nprint(\"Dataset class created successfully!\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def train_epoch(model, dataloader, optimizer, scheduler, device):\n    model.train()\n    total_loss = 0\n    predictions = []\n    true_labels = []\n    \n    progress_bar = tqdm(dataloader, desc='Training')\n    for batch in progress_bar:\n        optimizer.zero_grad()\n        \n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        labels = batch['labels'].to(device)\n        \n        outputs = model(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            labels=labels\n        )\n        \n        loss = outputs.loss\n        logits = outputs.logits\n        \n        total_loss += loss.item()\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n        optimizer.step()\n        scheduler.step()\n        \n        preds = torch.softmax(logits, dim=1)[:, 1].detach().cpu().numpy()\n        predictions.extend(preds)\n        true_labels.extend(labels.cpu().numpy())\n        \n        progress_bar.set_postfix({'loss': loss.item()})\n    \n    avg_loss = total_loss / len(dataloader)\n    auc = roc_auc_score(true_labels, predictions)\n    \n    return avg_loss, auc, predictions\n\ndef eval_model(model, dataloader, device):\n    model.eval()\n    predictions = []\n    true_labels = []\n    total_loss = 0\n    \n    with torch.no_grad():\n        for batch in tqdm(dataloader, desc='Evaluating'):\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            \n            if 'labels' in batch:\n                labels = batch['labels'].to(device)\n                outputs = model(\n                    input_ids=input_ids,\n                    attention_mask=attention_mask,\n                    labels=labels\n                )\n                total_loss += outputs.loss.item()\n                true_labels.extend(labels.cpu().numpy())\n            else:\n                outputs = model(\n                    input_ids=input_ids,\n                    attention_mask=attention_mask\n                )\n            \n            logits = outputs.logits\n            preds = torch.softmax(logits, dim=1)[:, 1].cpu().numpy()\n            predictions.extend(preds)\n    \n    if true_labels:\n        avg_loss = total_loss / len(dataloader)\n        auc = roc_auc_score(true_labels, predictions)\n        return predictions, auc, avg_loss\n    return predictions, None, None\n\nprint(\"Training and evaluation functions created successfully!\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Training configuration\nMAX_LENGTH = 512\nBATCH_SIZE = 4\nEPOCHS = 3\nLEARNING_RATE = 2e-5\nVAL_SPLIT = 0.1\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f'Using device: {device}')\n\n# X_large and y_large are already prepared and balanced from previous cell\nprint(f\"\\nUsing balanced dataset:\")\nprint(f\"Total samples: {len(X_large)}\")\nprint(f\"Human: {sum(y_large == 0)} ({sum(y_large == 0)/len(y_large)*100:.2f}%)\")\nprint(f\"AI: {sum(y_large == 1)} ({sum(y_large == 1)/len(y_large)*100:.2f}%)\")\n\n# Split large dataset\nX_train_large, X_val_large, y_train_large, y_val_large = train_test_split(\n    X_large, y_large, test_size=VAL_SPLIT, random_state=42, stratify=y_large\n)\n\nprint(f\"\\nTrain size: {len(X_train_large)}\")\nprint(f\"Val size: {len(X_val_large)}\")\n\n# Create datasets\ntrain_dataset = CheatingDetectionDataset(X_train_large, y_train_large, tokenizer, MAX_LENGTH)\nval_dataset = CheatingDetectionDataset(X_val_large, y_val_large, tokenizer, MAX_LENGTH)\n\ntrain_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\nval_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n\n# Initialize model\nmodel = DistilBertForSequenceClassification.from_pretrained(\n    model_path,\n    num_labels=2\n)\nmodel.to(device)\n\n# Optimizer and scheduler\noptimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\ntotal_steps = len(train_loader) * EPOCHS\nscheduler = get_linear_schedule_with_warmup(\n    optimizer,\n    num_warmup_steps=int(0.1 * total_steps),\n    num_training_steps=total_steps\n)\n\n# Training\nprint(\"\\n\" + \"=\"*60)\nprint(\"STAGE 1: Training on Large Balanced AI/Human Dataset\")\nprint(\"=\"*60)\n\nbest_val_loss = float('inf')\nbest_val_auc = 0\n\nfor epoch in range(EPOCHS):\n    print(f'\\n--- Epoch {epoch + 1}/{EPOCHS} ---')\n    \n    train_loss, train_auc, _ = train_epoch(model, train_loader, optimizer, scheduler, device)\n    val_preds, val_auc, val_loss = eval_model(model, val_loader, device)\n    \n    print(f'Train Loss: {train_loss:.4f}, Train AUC: {train_auc:.4f}')\n    print(f'Val Loss: {val_loss:.4f}, Val AUC: {val_auc:.4f}')\n    \n    if val_auc > best_val_auc:\n        best_val_auc = val_auc\n        best_val_loss = val_loss\n        torch.save(model.state_dict(), 'best_model_stage1.pth')\n        print(f'✓ Model saved! Best Val AUC: {best_val_auc:.4f}')\n\nprint(f\"\\n{'='*60}\")\nprint(f\"Stage 1 Complete - Best Val AUC: {best_val_auc:.4f}\")\nprint(f\"{'='*60}\")\n\n# Load best model\nmodel.load_state_dict(torch.load('best_model_stage1.pth'))\nprint(\"Loaded best model from Stage 1\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-10-12T00:57:24.672Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Plot prediction distribution\nplt.figure(figsize=(12, 5))\n\nplt.subplot(1, 2, 1)\nplt.hist(oof_predictions[y == 0], bins=30, alpha=0.5, label='Not Cheating')\nplt.hist(oof_predictions[y == 1], bins=30, alpha=0.5, label='Cheating')\nplt.xlabel('Prediction Probability')\nplt.ylabel('Count')\nplt.title('OOF Predictions Distribution')\nplt.legend()\n\nplt.subplot(1, 2, 2)\nplt.hist(test_predictions, bins=30, alpha=0.7)\nplt.xlabel('Prediction Probability')\nplt.ylabel('Count')\nplt.title('Test Predictions Distribution')\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"execution_failed":"2025-10-12T00:57:24.672Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create submission file\nsubmission = pd.DataFrame()\nsubmission['id'] = test_df['id']\nsubmission['is_cheating'] = test_predictions\n\n# Check submission format\nprint(\"Submission shape:\", submission.shape)\nprint(\"\\nSubmission preview:\")\nprint(submission.head(10))\nprint(\"\\nPrediction statistics:\")\nprint(submission['is_cheating'].describe())\n\n# Save submission\nsubmission.to_csv('submission50.csv', index=False)\nprint(\"\\nSubmission saved to 'submission50.csv'\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-10-12T00:57:24.672Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Classification metrics at threshold 0.5\nthreshold = 0.5\noof_binary = (oof_predictions > threshold).astype(int)\n\nprint(\"Classification Report (OOF Predictions):\")\nprint(\"=\"*50)\nprint(classification_report(y, oof_binary, target_names=['Not Cheating', 'Cheating']))\n\n# Confusion Matrix\ncm = confusion_matrix(y, oof_binary)\nplt.figure(figsize=(8, 6))\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\nplt.title('Confusion Matrix (OOF Predictions)')\nplt.ylabel('True Label')\nplt.xlabel('Predicted Label')\nplt.show()\n\nprint(f\"\\nFinal ROC-AUC Score: {overall_auc:.4f}\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-10-12T00:57:24.672Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dataset2 = pd.read_csv(\"/kaggle/input/ai-vs-human-text/AI_Human.csv\")\nprint(f\"Training set shape: {dataset2.shape}\")\n\n# Display first few rows\ndataset2.head()","metadata":{"trusted":true,"execution":{"execution_failed":"2025-10-12T00:57:24.672Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}